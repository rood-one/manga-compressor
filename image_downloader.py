import requests
import os
import logging
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from PIL import Image
import re

def wait_for_page_load(url, session, delay=5):
    """Ø§Ù†ØªØ¸Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø¨Ø´ÙƒÙ„ ÙƒØ§Ù…Ù„"""
    try:
        response = session.get(url)
        response.raise_for_status()
        logging.info(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø¨Ù†Ø¬Ø§Ø­ØŒ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± {delay} Ø«ÙˆØ§Ù†ÙŠ Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±...")
        time.sleep(delay)  # Ø§Ù†ØªØ¸Ø§Ø± Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±
        return response.content
    except Exception as e:
        logging.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©: {e}")
        return None

def find_image_urls(soup, base_url):
    """Ø§Ù„Ø¨Ø§Ø­Ø« Ø¹Ù† Ø¬Ù…ÙŠØ¹ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØµÙˆØ± ÙÙŠ Ø§Ù„ØµÙØ­Ø©"""
    image_urls = []
    
    # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ ÙˆØ³ÙˆÙ… img
    for img in soup.find_all('img'):
        for attr in ['src', 'data-src', 'data-original', 'data-source']:
            src = img.get(attr)
            if src:
                full_url = urljoin(base_url, src)
                if is_image_url(full_url):
                    image_urls.append(full_url)
    
    # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ ÙˆØ³ÙˆÙ… a (Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ù„ØµÙˆØ±)
    for link in soup.find_all('a', href=True):
        href = link['href']
        if is_image_url(href):
            full_url = urljoin(base_url, href)
            image_urls.append(full_url)
    
    # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ CSS background images
    for tag in soup.find_all(style=True):
        style = tag['style']
        urls = re.findall(r'url\([\'"]?(.*?)[\'"]?\)', style)
        for url in urls:
            full_url = urljoin(base_url, url)
            if is_image_url(full_url):
                image_urls.append(full_url)
    
    return list(set(image_urls))  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª

def is_image_url(url):
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø§Ø¨Ø· ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ ØµÙˆØ±Ø©"""
    image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp']
    return any(url.lower().endswith(ext) for ext in image_extensions)

def download_sequential_images(base_url, download_dir, session, max_images=100):
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ø±Ù‚Ù…ÙŠ (001.jpg, 002.jpg, Ø¥Ù„Ø®)"""
    downloaded_images = []
    
    for i in range(1, max_images + 1):
        # ØªÙ†Ø³ÙŠÙ‚Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ù„Ù„Ø£Ø³Ù…Ø§Ø¡
        filenames = [
            f"{i:03d}.jpg",
            f"{i:03d}.jpeg", 
            f"{i:03d}.png",
            f"{i}.jpg",
            f"{i}.jpeg",
            f"image_{i:03d}.jpg",
            f"img_{i:03d}.jpg",
            f"page_{i:03d}.jpg"
        ]
        
        for filename in filenames:
            image_url = urljoin(base_url, filename)
            try:
                response = session.get(image_url, timeout=10)
                if response.status_code == 200 and 'image' in response.headers.get('content-type', ''):
                    image_path = os.path.join(download_dir, f"{i:03d}.jpg")
                    
                    with open(image_path, 'wb') as f:
                        f.write(response.content)
                    
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ù„Ù ØµÙˆØ±Ø© ØµØ§Ù„Ø­Ø©
                    try:
                        with Image.open(image_path) as img:
                            img.verify()
                        downloaded_images.append(image_path)
                        logging.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„: {image_url}")
                        break  # Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ù„Ù„ØµÙˆØ±Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©
                    except Exception:
                        os.remove(image_path)  # Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ø§Ù„ØµØ§Ù„Ø­
                        continue
                        
            except Exception as e:
                continue
    
    return downloaded_images

def download_images(base_url, download_dir):
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±"""
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    })
    
    all_downloaded = []
    
    try:
        # Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©
        page_content = wait_for_page_load(base_url, session, delay=7)
        if not page_content:
            return []
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØµÙˆØ± ÙÙŠ Ø§Ù„ØµÙØ­Ø©
        soup = BeautifulSoup(page_content, 'html.parser')
        found_urls = find_image_urls(soup, base_url)
        
        logging.info(f"ğŸ” ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(found_urls)} Ø±Ø§Ø¨Ø· ØµÙˆØ±Ø© Ù…Ø­ØªÙ…Ù„ ÙÙŠ Ø§Ù„ØµÙØ­Ø©")
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø§Ù„ØªÙŠ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„ÙŠÙ‡Ø§
        for i, img_url in enumerate(found_urls):
            try:
                response = session.get(img_url, timeout=15)
                if response.status_code == 200 and 'image' in response.headers.get('content-type', ''):
                    filename = f"found_{i+1:03d}.jpg"
                    image_path = os.path.join(download_dir, filename)
                    
                    with open(image_path, 'wb') as f:
                        f.write(response.content)
                    
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµÙˆØ±Ø©
                    try:
                        with Image.open(image_path) as img:
                            img.verify()
                        all_downloaded.append(image_path)
                        logging.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ ØµÙˆØ±Ø© Ù…Ù† Ø§Ù„ØµÙØ­Ø©: {os.path.basename(img_url)}")
                    except Exception:
                        os.remove(image_path)
                        
            except Exception as e:
                logging.warning(f"âš ï¸ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ ØµÙˆØ±Ø© Ù…Ù† Ø§Ù„ØµÙØ­Ø©: {img_url}")
        
        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ ØµÙˆØ±Ø§Ù‹ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©ØŒ Ù†Ø¬Ø±Ø¨ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ©
        if not all_downloaded:
            logging.info("ğŸ”„ Ø¬Ø±Ø¨ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØµÙˆØ± Ø¨Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ø±Ù‚Ù…ÙŠ...")
            sequential_images = download_sequential_images(base_url, download_dir, session)
            all_downloaded.extend(sequential_images)
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„ØµÙˆØ± Ø­Ø³Ø¨ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡
        all_downloaded.sort()
        
    except Exception as e:
        logging.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ­Ù…ÙŠÙ„: {e}")
    
    logging.info(f"ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØµÙˆØ± Ø§Ù„ØªÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§: {len(all_downloaded)}")
    return all_downloaded
